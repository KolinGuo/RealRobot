{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:05, 18.36it/s]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "rosbag_dir = Path(\"/rl_benchmark/real_robot/rosbag_recordings\")\n",
    "rosbag_path = rosbag_dir / \"20230512_125925_test_camera_pose.bag\"\n",
    "\n",
    "# rs.log_to_console(rs.log_severity.debug)\n",
    "\n",
    "# Create pipeline and config from rosbag\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_device_from_file(str(rosbag_path), repeat_playback=False)\n",
    "\n",
    "# Start streaming from file\n",
    "profile = pipeline.start(config)\n",
    "playback = profile.get_device().as_playback()\n",
    "playback.set_real_time(False)\n",
    "\n",
    "# Get camera intrinsics\n",
    "depth_profile = profile.get_stream(rs.stream.depth).as_video_stream_profile()\n",
    "color_profile = profile.get_stream(rs.stream.color).as_video_stream_profile()\n",
    "color_intrinsics = color_profile.intrinsics\n",
    "width, height = color_intrinsics.width, color_intrinsics.height\n",
    "fx, fy = color_intrinsics.fx, color_intrinsics.fy\n",
    "cx, cy = color_intrinsics.ppx, color_intrinsics.ppy\n",
    "\n",
    "# Create an align object\n",
    "# rs.align allows us to perform alignment of depth frames to others frames\n",
    "# The \"align_to\" is the stream type to which we plan to align depth frames.\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "# Store the frames\n",
    "frames_dict = {\n",
    "    \"intrinsics\": np.array([fx, fy, cx, cy]),\n",
    "    \"rgb_image\": [],\n",
    "    \"depth_image\": [],\n",
    "}\n",
    "\n",
    "with tqdm() as pbar:\n",
    "    while True:\n",
    "        # Get time-synchronized frames of each enabled stream in the pipeline\n",
    "        frames_exist, frames = pipeline.try_wait_for_frames()\n",
    "        if not frames_exist:\n",
    "            break\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "        # Verify intrinsics\n",
    "        aligned_intrinsics = aligned_frames.get_profile().as_video_stream_profile().intrinsics\n",
    "        np.testing.assert_allclose(frames_dict[\"intrinsics\"],\n",
    "                                   [aligned_intrinsics.fx, aligned_intrinsics.fy,\n",
    "                                    aligned_intrinsics.ppx, aligned_intrinsics.ppy])\n",
    "\n",
    "        # Get aligned frames\n",
    "        depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "        # Use copy so frame resources can be released\n",
    "        depth_image = np.asanyarray(depth_frame.get_data()).copy()\n",
    "        color_image = np.asanyarray(color_frame.get_data()).copy()\n",
    "        \n",
    "        frames_dict[\"rgb_image\"].append(color_image)\n",
    "        frames_dict[\"depth_image\"].append(depth_image)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "for k, v in frames_dict.items():\n",
    "    frames_dict[k] = np.stack(v)\n",
    "np.savez_compressed('test.npz', **frames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('uint8'), dtype('uint16'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames_dict[\"rgb_image\"].dtype, frames_dict[\"depth_image\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "frames_dict = np.load(\"/rl_benchmark/real_robot/rosbag_recordings/20230512_125925_test_camera_pose.npz\")\n",
    "rgb_image = frames_dict[\"rgb_image\"][0]\n",
    "depth_image = frames_dict[\"depth_image\"][0]\n",
    "\n",
    "height, width = depth_image.shape\n",
    "\n",
    "uu, vv = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "z = depth_image / 1000.0\n",
    "x = (uu - cx) * z / fx\n",
    "y = (vv - cy) * z / fy\n",
    "xyz_image = np.stack([x, y, z], axis=-1)\n",
    "xyz_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = np.load(\"Tb_b2c.npy\")\n",
    "\n",
    "xyz_image = xyz_image.reshape(-1, 3) @ T[:3, :3].T + T[:3, 3]\n",
    "xyz_image = xyz_image.reshape(height, width, 3)\n",
    "xyz_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144e60b7edf345dd866b7bd65549d4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2352de8de991406a918162c0f101fb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7760cb40e504475c978329e3ec717088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2673f186f754489a92a530130f4973ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ba96b0dd654d538f7528d380a5eca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/rl_benchmark/grounded-sam/models/groundingdino_swinb_cogcoor.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgrounded_sam_track\u001b[39;00m \u001b[39mimport\u001b[39;00m GroundedSAMTrack\n\u001b[0;32m----> 2\u001b[0m grounded_sam_track \u001b[39m=\u001b[39m GroundedSAMTrack(device\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda:1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m ret \u001b[39m=\u001b[39m grounded_sam_track\u001b[39m.\u001b[39mpredict_and_track_batch([rgb_image], [\u001b[39m0\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mred cube\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgreen bowl\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m ret[\u001b[39m\"\u001b[39m\u001b[39mpred_masks\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grounded_sam_track/grounded_sam_track.py:448\u001b[0m, in \u001b[0;36mGroundedSAMTrack.__init__\u001b[0;34m(self, aot_model_variant, aot_max_len_long_term, num_trackers, predict_gap, prompt_with_robot_arm, tracking_device, *args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_gap \u001b[39m=\u001b[39m predict_gap\n\u001b[1;32m    446\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprompt_with_robot_arm \u001b[39m=\u001b[39m prompt_with_robot_arm\n\u001b[0;32m--> 448\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    450\u001b[0m \u001b[39m# Use separate device for mask tracking\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracking_device \u001b[39m=\u001b[39m tracking_device\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grounded_sam_track/grounded_sam_track.py:209\u001b[0m, in \u001b[0;36mGroundedSAM.__init__\u001b[0;34m(self, grounding_dino_model_variant, sam_model_variant, box_threshold, text_threshold, device)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[1;32m    208\u001b[0m \u001b[39m# Load grounding_dino_model and sam_model\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_grounding_dino_model()\n\u001b[1;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_sam_model(sam_model_variant)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/grounded_sam_track/grounded_sam_track.py:218\u001b[0m, in \u001b[0;36mGroundedSAM.load_grounding_dino_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m args\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrounding_dino_model \u001b[39m=\u001b[39m build_model(args)\n\u001b[0;32m--> 218\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrounding_dino_checkpoint,\n\u001b[1;32m    219\u001b[0m                         map_location\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    220\u001b[0m load_res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrounding_dino_model\u001b[39m.\u001b[39mload_state_dict(\n\u001b[1;32m    221\u001b[0m     clean_state_dict(checkpoint[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m]), strict\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[39mprint\u001b[39m(load_res)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/rl_benchmark/grounded-sam/models/groundingdino_swinb_cogcoor.pth'"
     ]
    }
   ],
   "source": [
    "from grounded_sam_track import GroundedSAMTrack\n",
    "grounded_sam_track = GroundedSAMTrack(device=\"cuda:1\")\n",
    "ret = grounded_sam_track.predict_and_track_batch([rgb_image], [0], [\"red cube\", \"green bowl\"])\n",
    "ret[\"pred_masks\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_active_profile().get_device().as_playback().is_real_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyrealsense2.[video_]stream_profile: Depth(0) 848x480 @ 30fps Z16>,\n",
       " <pyrealsense2.[video_]stream_profile: Color(0) 640x480 @ 30fps RGB8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_active_profile().get_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pyrl.utils.lib3d.o3d_utils import np2pcd\n",
    "pcd = np2pcd(xyz_image[mask], color_image[mask] / 255.0)\n",
    "import open3d as o3d\n",
    "coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame()\n",
    "o3d.visualization.draw_geometries([pcd, coord_frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699a0a7cdac3459e917884a048ab3c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Widget(value=\"<iframe src='http://localhost:46295/index.html?ui=P_0x7fc2886c0fd0_1&reconnect=auto' style='widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyvista as pv\n",
    "plotter = pv.Plotter(notebook=True)\n",
    "plotter.add_points(xyz_image[mask], point_size=1.0,\n",
    "                   scalars=color_image[mask] / 255.0, rgb=True)\n",
    "plotter.add_axes()\n",
    "plotter.add_bounding_box()\n",
    "plotter.show(jupyter_backend='trame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ 848x480  p[428.431 234.124]  f[428.159 428.159]  Brown Conrady [0 0 0 0 0] ],\n",
       " 848,\n",
       " 480)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stream profile and camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))\n",
    "depth_intrinsics = depth_profile.get_intrinsics()\n",
    "w, h = depth_intrinsics.width, depth_intrinsics.height\n",
    "depth_intrinsics, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([ 640x480  p[324.315 241.952]  f[601.441 601.377]  Inverse Brown Conrady [0 0 0 0 0] ],\n",
       " 640,\n",
       " 480)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get stream profile and camera intrinsics\n",
    "profile = pipeline.get_active_profile()\n",
    "color_profile = rs.video_stream_profile(profile.get_stream(rs.stream.color))\n",
    "color_intrinsics = color_profile.get_intrinsics()\n",
    "w, h = color_intrinsics.width, color_intrinsics.height\n",
    "color_intrinsics, w, h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
