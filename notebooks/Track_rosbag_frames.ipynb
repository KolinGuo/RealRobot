{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /rl_benchmark/real_robot/rosbag_recordings/20230516_215854_greenbox_posed_2_masks.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight'])\n",
      "\n",
      "Loading GroundingDINO: Took 3.606 seconds\n",
      "\n",
      "Loading SAM: Took 5.743 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2162 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:866: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "2023-05-19 00:46:55.480102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "  0%|          | 1/2162 [00:04<2:29:50,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2162/2162 [02:42<00:00, 13.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "rosbag_path = Path(\"../rosbag_recordings/20230516_215854_greenbox_posed_2.npz\").resolve()\n",
    "\n",
    "data = np.load(rosbag_path)\n",
    "rgb_images = data[\"rgb_image\"]\n",
    "depth_images = data[\"depth_image\"]\n",
    "pred_masks = []\n",
    "\n",
    "#T = np.load(\"/rl_benchmark/real_robot/rosbag_recordings/Tb_b2c.npy\")\n",
    "\n",
    "render_output_file = rosbag_path.parent / f\"{rosbag_path.stem}_masks.mp4\"\n",
    "print(f\"Saving to {render_output_file}\")\n",
    "\n",
    "import imageio\n",
    "out_video = imageio.get_writer(str(render_output_file), fps=5, quality=5, macro_block_size=4)\n",
    "\n",
    "from grounded_sam_track import GroundedSAMTrack\n",
    "grounded_sam_track = GroundedSAMTrack(predict_gap=9999)\n",
    "\n",
    "from tqdm import tqdm\n",
    "for frame_i, (rgb_image, depth_image) in enumerate(zip(tqdm(rgb_images), depth_images)):\n",
    "    ret = grounded_sam_track.predict_and_track_batch(\n",
    "        [rgb_image], [frame_i], [\"red cube\", \"green bowl\"]\n",
    "    )\n",
    "    pred_mask = ret[\"pred_masks\"][0]\n",
    "    pred_masks.append(pred_mask)\n",
    "    \n",
    "    # Save Video\n",
    "    from seg_and_track_anything.seg_track_anything import draw_mask, colorize_mask\n",
    "    from mani_skill2.utils.visualization.misc import tile_images\n",
    "\n",
    "    masked_images = []\n",
    "    masked_images.append(draw_mask(rgb_image, pred_mask))\n",
    "    masked_images.append(colorize_mask(pred_mask))\n",
    "    out_video.append_data(tile_images(masked_images))\n",
    "out_video.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import open3d as o3d\n",
    "from real_robot.utils.camera import depth2xyz, transform_points\n",
    "\n",
    "from pyrl.utils.lib3d import np2pcd\n",
    "\n",
    "def _process_pts(\n",
    "        pts_lst,\n",
    "        voxel_downsample_size, nb_neighbors, std_ratio\n",
    "    ):\n",
    "        if isinstance(pts_lst, np.ndarray):\n",
    "            pts_lst = [pts_lst]\n",
    "\n",
    "        ret_pts_lst = []\n",
    "        for pts in pts_lst:\n",
    "            pcd = np2pcd(pts)\n",
    "            if voxel_downsample_size is not None:\n",
    "                pcd = pcd.voxel_down_sample(voxel_size=voxel_downsample_size)\n",
    "            pcd_filter, inlier_inds = pcd.remove_statistical_outlier(\n",
    "                nb_neighbors=nb_neighbors, std_ratio=std_ratio\n",
    "            )\n",
    "            ret_pts_lst.append(np.asarray(pcd_filter.points))\n",
    "\n",
    "        if len(ret_pts_lst) == 1:\n",
    "            return ret_pts_lst[0]\n",
    "\n",
    "        return ret_pts_lst\n",
    "\n",
    "camera_pose = np.load(\"/rl_benchmark/real_robot/rosbag_recordings/Tb_b2c.npy\")\n",
    "\n",
    "rgb_image = rgb_images[0]\n",
    "depth_image = depth_images[0]\n",
    "pred_mask = pred_masks[0]\n",
    "xyz_image = depth2xyz(depth_image, *data[\"intrinsics\"])\n",
    "world_xyz_image = transform_points(xyz_image, camera_pose)\n",
    "\n",
    "cv2.namedWindow(\"Color / Depth\")\n",
    "cv2.imshow(\"Color / Depth\", rgb_image)\n",
    "cv2.waitKey(1)\n",
    "\n",
    "pcd_vis = o3d.visualization.Visualizer()\n",
    "pcd_vis.create_window(\"Point Cloud\", width=1280, height=720)\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(world_xyz_image.reshape(-1, 3))\n",
    "pcd.colors = o3d.utility.Vector3dVector(rgb_image.reshape(-1, 3) / 255.0)\n",
    "coord_frame = o3d.geometry.TriangleMesh().create_coordinate_frame()\n",
    "pcd_vis.add_geometry(coord_frame)\n",
    "pcd_vis.add_geometry(pcd)\n",
    "\n",
    "cube_pts = world_xyz_image[pred_mask == 1]\n",
    "cube_pts = _process_pts(\n",
    "    cube_pts, 0.005, 20, 0.005\n",
    ")\n",
    "bowl_pts = world_xyz_image[pred_mask == 2]\n",
    "bowl_pts = _process_pts(\n",
    "    bowl_pts, 0.005, 20, 0.005\n",
    ")\n",
    "cube_pos = np.mean(cube_pts, axis=0)\n",
    "bowl_pos = np.mean(bowl_pts, axis=0)\n",
    "# Extract bbox from object_pts\n",
    "bowl_mins, bowl_maxs = bowl_pts.min(0), bowl_pts.max(0)\n",
    "cube_mins, cube_maxs = cube_pts.min(0), cube_pts.max(0)\n",
    "\n",
    "cube_bbox_pos = np.mean([cube_mins, cube_maxs], axis=0)\n",
    "bowl_bbox_pos = np.mean([bowl_mins, bowl_maxs], axis=0)\n",
    "\n",
    "pcd_center = o3d.geometry.PointCloud()\n",
    "pcd_center.points = o3d.utility.Vector3dVector(np.stack([cube_pos, bowl_pos, cube_bbox_pos, bowl_bbox_pos], axis=0))\n",
    "pcd_center.colors = o3d.utility.Vector3dVector(np.stack([[0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0.]], axis=0))\n",
    "pcd_vis.add_geometry(pcd_center)\n",
    "\n",
    "cube_aabb = np2pcd(cube_pts).get_axis_aligned_bounding_box()\n",
    "bowl_aabb = np2pcd(bowl_pts).get_axis_aligned_bounding_box()\n",
    "pcd_vis.add_geometry(cube_aabb)\n",
    "pcd_vis.add_geometry(bowl_aabb)\n",
    "\n",
    "for i in range(1, len(rgb_images)):\n",
    "    rgb_image = rgb_images[i]\n",
    "    depth_image = depth_images[i]\n",
    "    pred_mask = pred_masks[i]\n",
    "\n",
    "    xyz_image = depth2xyz(depth_image, *data[\"intrinsics\"])\n",
    "    world_xyz_image = transform_points(xyz_image, camera_pose)\n",
    "    pcd.points = o3d.utility.Vector3dVector(world_xyz_image.reshape(-1, 3))\n",
    "    pcd.colors = o3d.utility.Vector3dVector(rgb_image.reshape(-1, 3) / 255.0)\n",
    "    \n",
    "\n",
    "    cube_pts = world_xyz_image[pred_mask == 1]\n",
    "    cube_pts = _process_pts(\n",
    "        cube_pts, 0.005, 20, 0.005\n",
    "    )\n",
    "    bowl_pts = world_xyz_image[pred_mask == 2]\n",
    "    bowl_pts = _process_pts(\n",
    "        bowl_pts, 0.005, 20, 0.005\n",
    "    )\n",
    "    cube_pos = np.mean(cube_pts, axis=0)\n",
    "    bowl_pos = np.mean(bowl_pts, axis=0)\n",
    "    # Extract bbox from object_pts\n",
    "    bowl_mins, bowl_maxs = bowl_pts.min(0), bowl_pts.max(0)\n",
    "    cube_mins, cube_maxs = cube_pts.min(0), cube_pts.max(0)\n",
    "\n",
    "    cube_bbox_pos = np.mean([cube_mins, cube_maxs], axis=0)\n",
    "    bowl_bbox_pos = np.mean([bowl_mins, bowl_maxs], axis=0)\n",
    "\n",
    "    pcd_center.points = o3d.utility.Vector3dVector(np.stack([cube_pos, bowl_pos, cube_bbox_pos, bowl_bbox_pos], axis=0))\n",
    "    pcd_center.colors = o3d.utility.Vector3dVector(np.stack([[0, 0, 1], [0, 0, 1], [0, 1, 0], [0, 1, 0.]], axis=0))\n",
    "    pcd_vis.update_geometry(pcd_center)\n",
    "\n",
    "    cube_aabb_new = np2pcd(cube_pts).get_axis_aligned_bounding_box()\n",
    "    bowl_aabb_new = np2pcd(bowl_pts).get_axis_aligned_bounding_box()\n",
    "    cube_aabb.max_bound = cube_aabb_new.max_bound\n",
    "    cube_aabb.min_bound = cube_aabb_new.min_bound\n",
    "    bowl_aabb.max_bound = bowl_aabb_new.max_bound\n",
    "    bowl_aabb.min_bound = bowl_aabb_new.min_bound\n",
    "    pcd_vis.update_geometry(cube_aabb)\n",
    "    pcd_vis.update_geometry(bowl_aabb)\n",
    "\n",
    "    pcd_vis.update_geometry(pcd)\n",
    "    pcd_vis.poll_events()\n",
    "    pcd_vis.update_renderer()\n",
    "\n",
    "    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "    cv2.imshow(\"Color / Depth\", np.hstack([cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR), depth_colormap]))\n",
    "    if cv2.waitKey(1) == 27:  # ESC\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "pcd_vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "Image.fromarray(tile_images(masked_images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
